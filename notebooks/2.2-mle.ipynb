{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:  2014 Period:  may-jun # 24700 cost:  22.134388 MaxGrad:  9.916512962961783e-05 MinEig:  1.3931268879695012e-10 MaxTheta:  5 MaxThetaValue:  4190.6836 alpha:  0.011\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import warnings\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "theta_list = []\n",
    "checkpoint_list = []\n",
    "\n",
    "\n",
    "\n",
    "year = [2014]\n",
    "period = ['may-jun'] # inputs: 'may-jun', 'jun-jul'\n",
    "\n",
    "for year in year:\n",
    "    \n",
    "    # Import data\n",
    "    if year == 2014:\n",
    "        X = np.load('../data/processed/data_2014.npz')\n",
    "        N = X['N']\n",
    "        \n",
    "    elif year == 2015:\n",
    "        X = np.load('../data/processed/data_2015.npz')\n",
    "        N = X['N']\n",
    "        \n",
    "    elif year == 2016:\n",
    "        X = np.load('../data/processed/data_2016.npz')\n",
    "        N = X['N']\n",
    "        \n",
    "    elif year == 2017:\n",
    "        X = np.load('../data/processed/data_2017.npz')\n",
    "        N = X['N']\n",
    "\n",
    "    dist = X['distance']\n",
    "    tI1 = X['tI1'].reshape(N,1)\n",
    "    tI2 = X['tI2'].reshape(N,1)\n",
    "    sI2 = X['sI2'].reshape(N,1)\n",
    "    \n",
    "    y_apr = X['y_apr'].reshape(N,1)\n",
    "    y_may = X['y_may'].reshape(N,1)\n",
    "    y_jun = X['y_jun'].reshape(N,1)\n",
    "    y_jul = X['y_jul'].reshape(N,1)\n",
    "\n",
    "    n_apr = X['n_apr'].reshape(N,1)\n",
    "    n_may = X['n_may'].reshape(N,1)\n",
    "    n_jun = X['n_jun'].reshape(N,1)\n",
    "    n_jul = X['n_jul'].reshape(N,1)\n",
    "\n",
    "    a_apr = X['a_apr'].reshape(N,1)\n",
    "    a_may = X['a_may'].reshape(N,1)\n",
    "    a_jun = X['a_jun'].reshape(N,1)\n",
    "    a_jul = X['a_jul'].reshape(N,1)\n",
    "\n",
    "    w_apr = X['wind_apr']\n",
    "    w_may = X['wind_may']\n",
    "    w_jun = X['wind_jun']\n",
    "    w_jul = X['wind_jul']\n",
    "\n",
    "    sI1_apr = X['sI1_apr'].reshape(N,1)\n",
    "    sI1_may = X['sI1_may'].reshape(N,1)\n",
    "    sI1_jun = X['sI1_jun'].reshape(N,1)\n",
    "    sI1_jul = X['sI1_jul'].reshape(N,1)\n",
    "\n",
    "    s_apr = X['s_apr'].reshape(N,1)\n",
    "    s_may = X['s_may'].reshape(N,1)\n",
    "    s_jun = X['s_jun'].reshape(N,1)\n",
    "    s_jul = X['s_jul'].reshape(N,1)\n",
    "\n",
    "\n",
    "    # Function to normalize the data\n",
    "    def norm(x):\n",
    "        \n",
    "        return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    \n",
    "    # Normalize the data\n",
    "    dist = norm(dist)\n",
    "    \n",
    "    a_apr = norm(a_apr)\n",
    "    a_may = norm(a_may)\n",
    "    a_jun = norm(a_jun)\n",
    "    a_jul = norm(a_jul)\n",
    "    \n",
    "    \n",
    "    for period in period:\n",
    "\n",
    "        if period == 'may-jun':\n",
    "            y = y_jun\n",
    "            n = n_jun\n",
    "            y_lag = y_may\n",
    "            n_lag = n_may\n",
    "            a_lag = a_may\n",
    "            w_lag = w_may\n",
    "            sI1_lag = sI1_may\n",
    "            s_lag = s_may\n",
    "        \n",
    "        elif period == 'jun-jul':\n",
    "            \n",
    "            y = y_jul\n",
    "            n = n_jul\n",
    "            y_lag = y_jun\n",
    "            n_lag = n_jun\n",
    "            a_lag = a_jun\n",
    "            w_lag = w_jun\n",
    "            sI1_lag = sI1_jun\n",
    "            s_lag = s_jun\n",
    "        \n",
    "\n",
    "        # Define the function eta() which takes input parameters theta and returns the log-odds of disease for each yard i in current time period\n",
    "        def eta(theta):\n",
    "                \n",
    "            beta1, beta2, delta1, delta2, gamma1, gamma2, alpha1, alpha2, eta11, eta12, eta21, eta22 = theta\n",
    "            \n",
    "            beta1_array = np.full((N,1), beta1)\n",
    "            beta2_array = np.full((N,1), beta2)\n",
    "            \n",
    "            auto_infection1 = delta1 * (y_lag / n_lag) * np.exp(-eta11 * s_lag)\n",
    "            auto_infection2 = delta2 * (y_lag / n_lag) * np.exp(-eta12 * s_lag)\n",
    "            \n",
    "            dispersal1 = []\n",
    "            dispersal2 = []\n",
    "            \n",
    "            for i in range(0, N):\n",
    "                \n",
    "                dispersal_array = ((a_lag * (y_lag / n_lag)) * (w_lag[:, i].reshape(N,1)))\n",
    "                dispersal_array1 = dispersal_array * np.exp(-eta21 * s_lag) * np.exp(-alpha1 * dist[:, i].reshape(N,1)) * sI1_lag\n",
    "                dispersal_array2 = dispersal_array * np.exp(-eta22 * s_lag) * np.exp(-alpha2 * dist[:, i].reshape(N,1)) * sI2\n",
    "                dispersal_component1_i = gamma1 * (np.sum(dispersal_array1) - dispersal_array1[i][0])\n",
    "                dispersal_component2_i = gamma2 * (np.sum(dispersal_array2) - dispersal_array2[i][0])\n",
    "                \n",
    "                dispersal1.append(dispersal_component1_i)\n",
    "                dispersal2.append(dispersal_component2_i)\n",
    "            \n",
    "            dispersal1 = np.array(dispersal1).reshape(N,1)\n",
    "            dispersal2 = np.array(dispersal2).reshape(N,1)\n",
    "            \n",
    "            eta = tI1 * (beta1_array + auto_infection1 + dispersal1) + tI2 * (beta2_array + auto_infection2 + dispersal2)\n",
    "            \n",
    "            return eta\n",
    "\n",
    "\n",
    "        def costFunction(theta): \n",
    "            \n",
    "            neg_log_likelihood = -(1/N) * np.sum(y * eta(theta) - n * np.log(1 + np.exp(eta(theta))))\n",
    "\n",
    "            return neg_log_likelihood\n",
    "\n",
    "\n",
    "        def partial(theta):\n",
    "            \n",
    "            beta1, beta2, delta1, delta2, gamma1, gamma2, alpha1, alpha2, eta11, eta12, eta21, eta22 = theta\n",
    "            \n",
    "            d_beta1 = tI1\n",
    "            d_beta2 = tI2\n",
    "            \n",
    "            d_delta1 = tI1 * (y_lag / n_lag) * np.exp(-eta11 * s_lag)\n",
    "            d_delta2 = tI2 * (y_lag / n_lag) * np.exp(-eta12 * s_lag)\n",
    "            \n",
    "            d_gamma1 = []\n",
    "            d_gamma2 = []\n",
    "            d_alpha1 = []\n",
    "            d_alpha2 = []\n",
    "            d_eta21 = []\n",
    "            d_eta22 = []\n",
    "            \n",
    "            for i in range(0, N):\n",
    "                \n",
    "                mask = np.arange(N) != i # mask out the current yard i\n",
    "            \n",
    "                d_gamma1_i = tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask])\n",
    "                d_gamma2_i = tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask])\n",
    "                \n",
    "                d_alpha1_i = -gamma1 * tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * dist[:, i][mask].reshape(N-1, 1))\n",
    "                d_alpha2_i = -gamma2 * tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * dist[:, i][mask].reshape(N-1, 1))\n",
    "                \n",
    "                d_eta21_i = -gamma1 * tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * s_lag[mask])\n",
    "                d_eta22_i = -gamma2 * tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * s_lag[mask])\n",
    "            \n",
    "                d_gamma1.append(d_gamma1_i)\n",
    "                d_gamma2.append(d_gamma2_i)\n",
    "                d_alpha1.append(d_alpha1_i)\n",
    "                d_alpha2.append(d_alpha2_i)\n",
    "                d_eta21.append(d_eta21_i)\n",
    "                d_eta22.append(d_eta22_i)\n",
    "            \n",
    "            d_gamma1 = np.array(d_gamma1).reshape(N,1)\n",
    "            d_gamma2 = np.array(d_gamma2).reshape(N,1)\n",
    "            d_alpha1 = np.array(d_alpha1).reshape(N,1)\n",
    "            d_alpha2 = np.array(d_alpha2).reshape(N,1)\n",
    "            d_eta21 = np.array(d_eta21).reshape(N,1)\n",
    "            d_eta22 = np.array(d_eta22).reshape(N,1)\n",
    "            \n",
    "            \n",
    "            d_eta11 = -tI1 * delta1 * s_lag * (y_lag / n_lag) * np.exp(-eta11 * s_lag)\n",
    "            d_eta12 = -tI2 * delta2 * s_lag * (y_lag / n_lag) * np.exp(-eta12 * s_lag)\n",
    "\n",
    "\n",
    "\n",
    "            grad_entries = np.array([d_beta1, d_beta2, d_delta1, d_delta2, d_gamma1, d_gamma2, d_alpha1, d_alpha2, d_eta11, d_eta12, d_eta21, d_eta22])\n",
    "            \n",
    "            return grad_entries\n",
    "\n",
    "        def partial_by_partial(theta):\n",
    "            \n",
    "            beta1, beta2, delta1, delta2, gamma1, gamma2, alpha1, alpha2, eta11, eta12, eta21, eta22 = theta\n",
    "            \n",
    "            d_beta1 = tI1\n",
    "            d_beta2 = tI2\n",
    "            \n",
    "            d_delta1 = tI1 * (y_lag / n_lag) * np.exp(-eta11 * s_lag)\n",
    "            d_delta2 = tI2 * (y_lag / n_lag) * np.exp(-eta12 * s_lag)\n",
    "            \n",
    "            d_eta11 = -tI1 * delta1 * s_lag * (y_lag / n_lag) * np.exp(-eta11 * s_lag)\n",
    "            d_eta12 = -tI2 * delta2 * s_lag * (y_lag / n_lag) * np.exp(-eta12 * s_lag)\n",
    "            \n",
    "            d_gamma1 = []\n",
    "            d_gamma2 = []\n",
    "            d_alpha1 = []\n",
    "            d_alpha2 = []\n",
    "            d_eta21 = []\n",
    "            d_eta22 = []\n",
    "            \n",
    "            for i in range(0, N):\n",
    "                \n",
    "                mask = np.arange(N) != i # mask out the current yard i\n",
    "            \n",
    "                d_gamma1_i = tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask])\n",
    "                d_gamma2_i = tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask])\n",
    "                \n",
    "                d_alpha1_i = -gamma1 * tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * dist[:, i][mask].reshape(N-1, 1))\n",
    "                d_alpha2_i = -gamma2 * tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * dist[:, i][mask].reshape(N-1, 1))\n",
    "\n",
    "                d_eta21_i = -gamma1 * tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * s_lag[mask])\n",
    "                d_eta22_i = -gamma2 * tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * s_lag[mask])\n",
    "                \n",
    "                d_gamma1.append(d_gamma1_i)\n",
    "                d_gamma2.append(d_gamma2_i)\n",
    "                d_alpha1.append(d_alpha1_i)\n",
    "                d_alpha2.append(d_alpha2_i)\n",
    "                d_eta21.append(d_eta21_i)\n",
    "                d_eta22.append(d_eta22_i)\n",
    "            \n",
    "            d_gamma1 = np.array(d_gamma1).reshape(N,1)\n",
    "            d_gamma2 = np.array(d_gamma2).reshape(N,1)\n",
    "            d_alpha1 = np.array(d_alpha1).reshape(N,1)\n",
    "            d_alpha2 = np.array(d_alpha2).reshape(N,1)\n",
    "            d_eta21 = np.array(d_eta21).reshape(N,1)\n",
    "            d_eta22 = np.array(d_eta22).reshape(N,1)\n",
    "                \n",
    "\n",
    "            grad_entries = np.array([[d_beta1*d_beta1, d_beta2*d_beta1, d_delta1*d_beta1, d_delta2*d_beta1, d_gamma1*d_beta1, d_gamma2*d_beta1, d_alpha1*d_beta1, d_alpha2*d_beta1, d_eta11*d_beta1, d_eta12*d_beta1, d_eta21*d_beta1, d_eta22*d_beta1],\n",
    "                                    [d_beta1*d_beta2, d_beta2*d_beta2, d_delta1*d_beta2, d_delta2*d_beta2, d_gamma1*d_beta2, d_gamma2*d_beta2, d_alpha1*d_beta2, d_alpha2*d_beta2, d_eta11*d_beta2, d_eta12*d_beta2, d_eta21*d_beta2, d_eta22*d_beta2],\n",
    "                                    [d_beta1*d_delta1, d_beta2*d_delta1, d_delta1*d_delta1, d_delta2*d_delta1, d_gamma1*d_delta1, d_gamma2*d_delta1, d_alpha1*d_delta1, d_alpha2*d_delta1, d_eta11*d_delta1, d_eta12*d_delta1, d_eta21*d_delta1, d_eta22*d_delta1],\n",
    "                                    [d_beta1*d_delta2, d_beta2*d_delta2, d_delta1*d_delta2, d_delta2*d_delta2, d_gamma1*d_delta2, d_gamma2*d_delta2, d_alpha1*d_delta2, d_alpha2*d_delta2, d_eta11*d_delta2, d_eta12*d_delta2, d_eta21*d_delta2, d_eta22*d_delta2],\n",
    "                                    [d_beta1*d_gamma1, d_beta2*d_gamma1, d_delta1*d_gamma1, d_delta2*d_gamma1, d_gamma1*d_gamma1, d_gamma2*d_gamma1, d_alpha1*d_gamma1, d_alpha2*d_gamma1, d_eta11*d_gamma1, d_eta12*d_gamma1, d_eta21*d_gamma1, d_eta22*d_gamma1],\n",
    "                                    [d_beta1*d_gamma2, d_beta2*d_gamma2, d_delta1*d_gamma2, d_delta2*d_gamma2, d_gamma1*d_gamma2, d_gamma2*d_gamma2, d_alpha1*d_gamma2, d_alpha2*d_gamma2, d_eta11*d_gamma2, d_eta12*d_gamma2, d_eta21*d_gamma2, d_eta22*d_gamma2],\n",
    "                                    [d_beta1*d_alpha1, d_beta2*d_alpha1, d_delta1*d_alpha1, d_delta2*d_alpha1, d_gamma1*d_alpha1, d_gamma2*d_alpha1, d_alpha1*d_alpha1, d_alpha2*d_alpha1, d_eta11*d_alpha1, d_eta12*d_alpha1, d_eta21*d_alpha1, d_eta22*d_alpha1],\n",
    "                                    [d_beta1*d_alpha2, d_beta2*d_alpha2, d_delta1*d_alpha2, d_delta2*d_alpha2, d_gamma1*d_alpha2, d_gamma2*d_alpha2, d_alpha1*d_alpha2, d_alpha2*d_alpha2, d_eta11*d_alpha2, d_eta12*d_alpha2, d_eta21*d_alpha2, d_eta22*d_alpha2],\n",
    "                                    [d_beta1*d_eta11, d_beta2*d_eta11, d_delta1*d_eta11, d_delta2*d_eta11, d_gamma1*d_eta11, d_gamma2*d_eta11, d_alpha1*d_eta11, d_alpha2*d_eta11, d_eta11*d_eta11, d_eta12*d_eta11, d_eta21*d_eta11, d_eta22*d_eta11],\n",
    "                                    [d_beta1*d_eta12, d_beta2*d_eta12, d_delta1*d_eta12, d_delta2*d_eta12, d_gamma1*d_eta12, d_gamma2*d_eta12, d_alpha1*d_eta12, d_alpha2*d_eta12, d_eta11*d_eta12, d_eta12*d_eta12, d_eta21*d_eta12, d_eta22*d_eta12],\n",
    "                                    [d_beta1*d_eta21, d_beta2*d_eta21, d_delta1*d_eta21, d_delta2*d_eta21, d_gamma1*d_eta21, d_gamma2*d_eta21, d_alpha1*d_eta21, d_alpha2*d_eta21, d_eta11*d_eta21, d_eta12*d_eta21, d_eta21*d_eta21, d_eta22*d_eta21],\n",
    "                                    [d_beta1*d_eta22, d_beta2*d_eta22, d_delta1*d_eta22, d_delta2*d_eta22, d_gamma1*d_eta22, d_gamma2*d_eta22, d_alpha1*d_eta22, d_alpha2*d_eta22, d_eta11*d_eta22, d_eta12*d_eta22, d_eta21*d_eta22, d_eta22*d_eta22]])\n",
    "            \n",
    "            \n",
    "            \n",
    "            return grad_entries\n",
    "\n",
    "        def partial_sq(theta):\n",
    "            \n",
    "            beta1, beta2, delta1, delta2, gamma1, gamma2, alpha1, alpha2, eta11, eta12, eta21, eta22 = theta\n",
    "            \n",
    "            # delta1 second derivatives\n",
    "            \n",
    "            d_delta1_d_eta11 = -tI1 * (y_lag / n_lag) * np.exp(-eta11 * s_lag) * s_lag\n",
    "            d_delta1_d_eta12 = 0\n",
    "            d_delta2_d_eta11 = 0\n",
    "            d_delta2_d_eta12 = -tI2 * (y_lag / n_lag) * np.exp(-eta12 * s_lag) * s_lag\n",
    "            d_gamma1_d_eta22 = 0\n",
    "            d_gamma1_d_alpha2 = 0\n",
    "            d_gamma2_d_eta21 = 0\n",
    "            d_gamma2_d_alpha1 = 0\n",
    "            d_alpha1_d_gamma2 = 0\n",
    "            d_alpha1_d_eta22 = 0\n",
    "            d_alpha1_d_alpha2 = 0\n",
    "            d_alpha2_d_gamma1 = 0\n",
    "            d_alpha2_d_eta21 = 0\n",
    "            d_alpha2_d_alpha1 = 0\n",
    "            d_eta11_d_delta1 = -tI1 * s_lag * (y_lag / n_lag) * np.exp(-eta11 * s_lag)\n",
    "            d_eta11_d_delta2 = 0\n",
    "            d_eta11_d_eta11 = tI1 * delta1 * (s_lag**2) * (y_lag / n_lag) * np.exp(-eta11 * s_lag)\n",
    "            d_eta11_d_eta12 = 0\n",
    "            d_eta12_d_delta1 = 0\n",
    "            d_eta12_d_delta2 = -tI2 * s_lag * (y_lag / n_lag) * np.exp(-eta12 * s_lag)\n",
    "            d_eta12_d_eta11 = 0\n",
    "            d_eta12_d_eta12 = tI2 * delta2 * (s_lag**2) * (y_lag / n_lag) * np.exp(-eta12 * s_lag)\n",
    "            d_eta21_d_gamma2 = 0\n",
    "            d_eta21_d_eta22 = 0\n",
    "            d_eta21_d_alpha2 = 0\n",
    "            d_eta22_d_gamma1 = 0\n",
    "            d_eta22_d_eta21 = 0\n",
    "            d_eta22_d_alpha1 = 0\n",
    "            \n",
    "            # summations\n",
    "            \n",
    "            d_gamma1_d_eta21 = []\n",
    "            d_gamma1_d_alpha1 = []\n",
    "            d_gamma2_d_eta22 = []\n",
    "            d_gamma2_d_alpha2 = []\n",
    "            d_alpha1_d_gamma1 = []\n",
    "            d_alpha1_d_eta21 = []\n",
    "            d_alpha1_d_alpha1 = []\n",
    "            d_alpha2_d_gamma2 = []\n",
    "            d_alpha2_d_eta22 = []\n",
    "            d_alpha2_d_alpha2 = []\n",
    "            d_eta21_d_gamma1 = []\n",
    "            d_eta21_d_eta21 = []\n",
    "            d_eta21_d_alpha1 = []\n",
    "            d_eta22_d_gamma2 = []\n",
    "            d_eta22_d_eta22 = []\n",
    "            d_eta22_d_alpha2 = []\n",
    "            \n",
    "            \n",
    "            for i in range(0, N):\n",
    "                \n",
    "                mask = np.arange(N) != i # mask out the current yard i\n",
    "                \n",
    "                d_gamma1_d_eta21_i = -tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * s_lag[mask])\n",
    "                d_gamma1_d_alpha1_i = -tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * dist[:, i][mask].reshape(N-1, 1))\n",
    "                d_gamma2_d_eta22_i = -tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * s_lag[mask])\n",
    "                d_gamma2_d_alpha2_i = -tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * dist[:, i][mask].reshape(N-1, 1))\n",
    "                d_alpha1_d_gamma1_i = -tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * dist[:, i][mask].reshape(N-1, 1))\n",
    "                d_alpha1_d_eta21_i = gamma1 * tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * dist[:, i][mask].reshape(N-1, 1) * s_lag[mask])\n",
    "                d_alpha1_d_alpha1_i = gamma1 * tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * (dist[:, i][mask].reshape(N-1, 1))**2)\n",
    "                d_alpha2_d_gamma2_i = -tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * dist[:, i][mask].reshape(N-1, 1))\n",
    "                d_alpha2_d_eta22_i = gamma2 * tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * dist[:, i][mask].reshape(N-1, 1) * s_lag[mask])\n",
    "                d_alpha2_d_alpha2_i = gamma2 * tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * (dist[:, i][mask].reshape(N-1, 1))**2)\n",
    "                d_eta21_d_gamma1_i = -tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * s_lag[mask])\n",
    "                d_eta21_d_eta21_i = gamma1 * tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * (s_lag[mask]**2))\n",
    "                d_eta21_d_alpha1_i = gamma1 * tI1[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta21 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha1 * dist[:, i][mask].reshape(N-1, 1))) * sI1_lag[mask] * s_lag[mask] * (dist[:, i][mask].reshape(N-1, 1)))\n",
    "                d_eta22_d_gamma2_i = -tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * s_lag[mask])\n",
    "                d_eta22_d_eta22_i = gamma2 * tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * (s_lag[mask]**2))\n",
    "                d_eta22_d_alpha2_i = gamma2 * tI2[i] * np.sum((a_lag[mask] * (y_lag[mask] / n_lag[mask])) * np.exp(-eta22 * s_lag[mask]) * (w_lag[:, i][mask].reshape(N-1, 1) * np.exp(-alpha2 * dist[:, i][mask].reshape(N-1, 1))) * sI2[mask] * s_lag[mask] * (dist[:, i][mask].reshape(N-1, 1)))\n",
    "\n",
    "                d_gamma1_d_eta21.append(d_gamma1_d_eta21_i)\n",
    "                d_gamma1_d_alpha1.append(d_gamma1_d_alpha1_i)\n",
    "                d_gamma2_d_eta22.append(d_gamma2_d_eta22_i)\n",
    "                d_gamma2_d_alpha2.append(d_gamma2_d_alpha2_i)\n",
    "                d_alpha1_d_gamma1.append(d_alpha1_d_gamma1_i)\n",
    "                d_alpha1_d_eta21.append(d_alpha1_d_eta21_i)\n",
    "                d_alpha1_d_alpha1.append(d_alpha1_d_alpha1_i)\n",
    "                d_alpha2_d_gamma2.append(d_alpha2_d_gamma2_i)\n",
    "                d_alpha2_d_eta22.append(d_alpha2_d_eta22_i)\n",
    "                d_alpha2_d_alpha2.append(d_alpha2_d_alpha2_i)\n",
    "                d_eta21_d_gamma1.append(d_eta21_d_gamma1_i)\n",
    "                d_eta21_d_eta21.append(d_eta21_d_eta21_i)\n",
    "                d_eta21_d_alpha1.append(d_eta21_d_alpha1_i)\n",
    "                d_eta22_d_gamma2.append(d_eta22_d_gamma2_i)\n",
    "                d_eta22_d_eta22.append(d_eta22_d_eta22_i)\n",
    "                d_eta22_d_alpha2.append(d_eta22_d_alpha2_i)\n",
    "                \n",
    "            d_gamma1_d_eta21 = np.array(d_gamma1_d_eta21).reshape((N, 1))\n",
    "            d_gamma1_d_alpha1 = np.array(d_gamma1_d_alpha1).reshape((N, 1))\n",
    "            d_gamma2_d_eta22 = np.array(d_gamma2_d_eta22).reshape((N, 1))\n",
    "            d_gamma2_d_alpha2 = np.array(d_gamma2_d_alpha2).reshape((N, 1))\n",
    "            d_alpha1_d_gamma1 = np.array(d_alpha1_d_gamma1).reshape((N, 1))\n",
    "            d_alpha1_d_eta21 = np.array(d_alpha1_d_eta21).reshape((N, 1))\n",
    "            d_alpha1_d_alpha1 = np.array(d_alpha1_d_alpha1).reshape((N, 1))\n",
    "            d_alpha2_d_gamma2 = np.array(d_alpha2_d_gamma2).reshape((N, 1))\n",
    "            d_alpha2_d_eta22 = np.array(d_alpha2_d_eta22).reshape((N, 1))\n",
    "            d_alpha2_d_alpha2 = np.array(d_alpha2_d_alpha2).reshape((N, 1))\n",
    "            d_eta21_d_gamma1 = np.array(d_eta21_d_gamma1).reshape((N, 1))\n",
    "            d_eta21_d_eta21 = np.array(d_eta21_d_eta21).reshape((N, 1))\n",
    "            d_eta21_d_alpha1 = np.array(d_eta21_d_alpha1).reshape((N, 1))\n",
    "            d_eta22_d_gamma2 = np.array(d_eta22_d_gamma2).reshape((N, 1))\n",
    "            d_eta22_d_eta22 = np.array(d_eta22_d_eta22).reshape((N, 1))\n",
    "            d_eta22_d_alpha2 = np.array(d_eta22_d_alpha2).reshape((N, 1))\n",
    "                \n",
    "                \n",
    "            \n",
    "            zero = np.zeros((N, 1))\n",
    "            \n",
    "            hess_entries = np.array([[zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero],    #d_beta1\n",
    "                                    [zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero],    #d_beta2\n",
    "                                    [zero, zero, zero, zero, zero, zero, zero, zero, d_eta11_d_delta1, zero, zero, zero],    #d_delta1\n",
    "                                    [zero, zero, zero, zero, zero, zero, zero, zero, zero, d_eta12_d_delta2, zero, zero],    #d_delta2\n",
    "                                    [zero, zero, zero, zero, zero, zero, d_alpha1_d_gamma1, zero, zero, zero, d_eta21_d_gamma1, zero],    #d_gamma1\n",
    "                                    [zero, zero, zero, zero, zero, zero, zero, d_alpha2_d_gamma2, zero, zero, zero, d_eta22_d_gamma2],    #d_gamma2\n",
    "                                    [zero, zero, zero, zero, d_gamma1_d_alpha1, zero, d_alpha1_d_alpha1, zero, zero, zero, d_eta21_d_alpha1, zero],    #d_alpha1\n",
    "                                    [zero, zero, zero, zero, zero, d_gamma2_d_alpha2, zero, d_alpha2_d_alpha2, zero, zero, zero, d_eta22_d_alpha2],    #d_alpha2\n",
    "                                    [zero, zero, d_delta1_d_eta11, zero, zero, zero, zero, zero, d_eta11_d_eta11, zero, zero, zero],    #d_eta11\n",
    "                                    [zero, zero, zero, d_delta2_d_eta12, zero, zero, zero, zero, zero, d_eta12_d_eta12, zero, zero],    #d_eta12\n",
    "                                    [zero, zero, zero, zero, d_gamma1_d_eta21, zero, d_alpha1_d_eta21, zero, zero, zero, d_eta21_d_eta21, zero],    #d_eta21\n",
    "                                    [zero, zero, zero, zero, zero, d_gamma2_d_eta22, zero, d_alpha2_d_eta22, zero, zero, zero, d_eta22_d_eta22]])   #d_eta22\n",
    "            \n",
    "            \n",
    "            return hess_entries\n",
    "\n",
    "\n",
    "\n",
    "        # Gradient entries\n",
    "        def gradient(theta):\n",
    "            \n",
    "            mu = y - (n / (1 + np.exp(-eta(theta))))\n",
    "            \n",
    "            # Gradient \n",
    "            gradient = - (1 / N) * np.sum((partial(theta) * mu), axis=1)\n",
    "            \n",
    "            return gradient\n",
    "        \n",
    "        # Gradient entries\n",
    "        def gradient1(theta):\n",
    "            \n",
    "            mu = y - (n / (1 + np.exp(-eta(theta))))\n",
    "            \n",
    "            # Gradient \n",
    "            gradient1 = - (1 / N) * np.sum((partial(theta) * mu), axis=1)\n",
    "            \n",
    "            return gradient1.ravel()\n",
    "\n",
    "\n",
    "        # Hessian\n",
    "        def hessian(theta):\n",
    "            \n",
    "            mu = y - (n / (1 + np.exp(-eta(theta))))\n",
    "            \n",
    "            # Hessian entries\n",
    "            hessian = - (1 / N) * np.sum((partial_sq(theta) * mu - n * (partial_by_partial(theta)) * (np.exp(-eta(theta)) / (1 + np.exp(-eta(theta)))**2)), axis=2)\n",
    "            hessian = hessian.reshape((12, 12))\n",
    "            \n",
    "            return hessian\n",
    "        \n",
    "        \n",
    "        # Adam optimizer\n",
    "        def adam(theta, alpha, num_iters, tolerance, b_1=0.9, b_2=0.999, eps=1e-8, clip_norm=100.0):\n",
    "            theta = theta.copy()\n",
    "            J_history = []\n",
    "            m = np.zeros(theta.shape)\n",
    "            v = np.zeros(theta.shape)\n",
    "            \n",
    "            for i in range(num_iters):\n",
    "                \n",
    "                g = gradient(theta)\n",
    "\n",
    "                # Gradient clipping\n",
    "                g_norm = np.linalg.norm(g)\n",
    "                if g_norm > clip_norm:\n",
    "                    g = (g / g_norm) * clip_norm\n",
    "                \n",
    "                m = b_1 * m + (1 - b_1) * g\n",
    "                v = b_2 * v + (1 - b_2) * g**2\n",
    "                mhat = m / (1 - b_1**(i+1))\n",
    "                vhat = v / (1 - b_2**(i+1))\n",
    "                \n",
    "                change = alpha * mhat / (np.sqrt(vhat) + eps)\n",
    "                theta = theta - change\n",
    "                \n",
    "                J_history.append(costFunction(theta))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    \n",
    "                    max_abs_grad = np.max(np.abs(g)) \n",
    "                    min_eig_hess = np.min(np.linalg.eigvalsh(hessian(theta)))\n",
    "                    print('Year: ', year, 'Period: ', period, '#', i , 'cost: ', np.round(costFunction(theta), 6), 'MaxGrad: ', max_abs_grad, 'MinEig: ', min_eig_hess, 'MaxTheta: ', np.argmax(np.abs(theta)), 'MaxThetaValue: ', np.round(np.max(np.abs(theta)),4), end='\\r')\n",
    "                    \n",
    "                    if (max_abs_grad <= tolerance) and (min_eig_hess > 0):\n",
    "                        break\n",
    "                \n",
    "            return theta, J_history, g \n",
    "        \n",
    "        \n",
    "        # Adam optimizer with step decay\n",
    "        def adam_with_decay(theta, initial_alpha, num_iters, decay_rate, decay_steps, tolerance, b_1=0.9, b_2=0.999, eps=1e-8, clip_norm=0.5):\n",
    "            theta = theta.copy()\n",
    "            J_history = []\n",
    "            m = np.zeros(theta.shape)\n",
    "            v = np.zeros(theta.shape)\n",
    "            alpha = initial_alpha\n",
    "\n",
    "            for i in range(num_iters):\n",
    "\n",
    "                # decay the learning rate every few steps\n",
    "                if i % decay_steps == 0 and i != 0:\n",
    "                    alpha *= decay_rate\n",
    "\n",
    "                g = gradient(theta)\n",
    "\n",
    "                # Gradient clipping\n",
    "                g_norm = np.linalg.norm(g)\n",
    "                if g_norm > clip_norm:\n",
    "                    g = (g / g_norm) * clip_norm\n",
    "\n",
    "                m = b_1 * m + (1 - b_1) * g\n",
    "                v = b_2 * v + (1 - b_2) * g**2\n",
    "                mhat = m / (1 - b_1**(i+1))\n",
    "                vhat = v / (1 - b_2**(i+1))\n",
    "\n",
    "                change = alpha * mhat / (np.sqrt(vhat) + eps)\n",
    "                theta = theta - change\n",
    "\n",
    "                J_history.append(costFunction(theta))\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    max_abs_grad = np.max(np.abs(g)) \n",
    "                    min_eig_hess = np.min(np.linalg.eigvalsh(hessian(theta)))\n",
    "                    print('Year: ', year, 'Period: ', period, '#', i , 'cost: ', np.round(costFunction(theta), 6), 'MaxGrad: ', max_abs_grad, 'MinEig: ', min_eig_hess, 'MaxTheta: ', np.argmax(np.abs(theta)), 'MaxThetaValue: ', np.round(np.max(np.abs(theta)),4), 'alpha: ', alpha, end='\\r')\n",
    "\n",
    "                    \n",
    "                    if (year == 2014) & (period == 'may-jun'):\n",
    "            \n",
    "                        np.save('../reports/parameters/theta_may-jun_2014_1.npy', theta)\n",
    "                    \n",
    "                    elif (year == 2014) & (period == 'jun-jul'):\n",
    "                        \n",
    "                        np.save('../reports/parameters/theta_jun-jul_2014_1.npy', theta)\n",
    "                        \n",
    "                    elif (year == 2015) & (period == 'may-jun'):\n",
    "                        \n",
    "                        np.save('../reports/parameters/theta_may-jun_2015_1.npy', theta)\n",
    "                        \n",
    "                    elif (year == 2015) & (period == 'jun-jul'):\n",
    "                        \n",
    "                        np.save('../reports/parameters/theta_jun-jul_2015_1.npy', theta)\n",
    "                        \n",
    "                    elif (year == 2016) & (period == 'may-jun'):\n",
    "                        \n",
    "                        np.save('../reports/parameters/theta_may-jun_2016_1.npy', theta)\n",
    "                        \n",
    "                    elif (year == 2016) & (period == 'jun-jul'):\n",
    "                        \n",
    "                        np.save('../reports/parameters/theta_jun-jul_2016_1.npy', theta)\n",
    "                        \n",
    "                    elif (year == 2017) & (period == 'may-jun'):\n",
    "                        \n",
    "                        np.save('../reports/parameters/theta_may-jun_2017_1.npy', theta)\n",
    "                        \n",
    "                    elif (year == 2017) & (period == 'jun-jul'):\n",
    "                        \n",
    "                        np.save('../reports/parameters/theta_jun-jul_2017_1.npy', theta)\n",
    "                    \n",
    "                    \n",
    "                    if (max_abs_grad <= tolerance):\n",
    "                        break\n",
    "\n",
    "            return theta, J_history, g\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        # Initialize fitting parameters\n",
    "        theta = np.zeros((12,1))  # Create an array of zeros\n",
    "        theta[2:] = np.random.normal(0, 100 / np.sqrt(N), size=(10, 1))  # Fill the rest with random numbers\n",
    "\n",
    "        \n",
    "        #while True:\n",
    "            # Initialize fitting parameters\n",
    "        #    theta = np.random.normal(0, 2 / np.sqrt(N), size=(12, 1))\n",
    "            \n",
    "        #    try:\n",
    "                # Minimize the cost function and get the optimized parameter values\n",
    "                #res = minimize(costFunction, theta, method='BFGS', jac=gradient1, hess=hessian, options={'gtol': 1e-5, 'maxiter': 100000, 'disp': True})\n",
    "        #        res = minimize(costFunction, theta, method='BFGS', options={'gtol': 1e-5, 'maxiter': 100000, 'disp': True})\n",
    "        #        theta = res.x\n",
    "                \n",
    "        #        max_abs_grad = np.max(np.abs(gradient(theta)))\n",
    "        #        min_eig_hess = np.min(np.linalg.eigvalsh(hessian(theta)))\n",
    "                \n",
    "        #        print('Year: ', year, 'Period: ', period, 'MaxGrad: ', max_abs_grad, 'MinEig: ', min_eig_hess, end='\\r')\n",
    "                \n",
    "                # Check custom convergence criteria\n",
    "                #if (max_abs_grad < 1e-5) and (min_eig_hess > 0) and np.max(np.abs(theta)) < 500000:\n",
    "        #        if (max_abs_grad < 1e-5) and np.max(np.abs(theta)) < 500000:\n",
    "        #            break\n",
    "        #    except np.linalg.LinAlgError:\n",
    "        #        print('LinAlgError: eigenvalues did not converge. Retrying...')\n",
    "        \n",
    "        if year == 2014:\n",
    "            \n",
    "            alpha = 0.01\n",
    "            tolerance = 1e-4\n",
    "            theta = np.load('../reports/parameters/theta_may-jun_2014_SLSQP.npy', allow_pickle=True)\n",
    "        \n",
    "        if year == 2015:\n",
    "            \n",
    "            alpha = 1\n",
    "            tolerance = 1e-5\n",
    "            #theta = np.random.normal(0, 2 / np.sqrt(N), size=(12, 1))\n",
    "        \n",
    "        elif year == 2016:\n",
    "        \n",
    "            alpha = 1\n",
    "            tolerance = 1e-5\n",
    "            #theta = np.random.normal(0, 2 / np.sqrt(N), size=(12, 1))\n",
    "        \n",
    "        elif year == 2017:\n",
    "            \n",
    "            alpha = 0.05\n",
    "            tolerance = 1e-5\n",
    "            \n",
    "            # Initialize fitting parameters\n",
    "            #theta = np.random.normal(-0.5, 0.5, size=(12, 1))\n",
    "            \n",
    "            # Minimize the cost function and get the optimized initial parameter values\n",
    "            #res = minimize(costFunction, theta, method='BFGS', jac=gradient1, hess=hessian, options={'gtol': 1e-8, 'maxiter': 100000, 'disp': True})\n",
    "            #theta = res.x\n",
    "            #theta = theta.reshape(12, 1)\n",
    "            \n",
    "            #if period == 'may-jun':\n",
    "                \n",
    "            #    theta = np.load('../reports/parameters/theta_may-jun_2017_num_1.npy', allow_pickle=True)\n",
    "            \n",
    "            #elif period == 'jun-jul':\n",
    "                \n",
    "            #    theta = np.load('../reports/parameters/theta_jun-jul_2017_num_1.npy', allow_pickle=True)\n",
    "                \n",
    "            #theta = theta.reshape(12, 1)\n",
    "            \n",
    "        #theta = np.load('../reports/theta2.npy')\n",
    "\n",
    "        # Gradient descent settings\n",
    "        iterations = 5000000\n",
    "\n",
    "        theta, J_history, g = adam_with_decay(theta, initial_alpha=alpha, num_iters=iterations, decay_rate=1, decay_steps=5000, tolerance=tolerance)\n",
    "\n",
    "        #print('iteration start:\\t{:.3f}'.format(np.int32(checkpoint[1])))\n",
    "        #print('previous final cost:\\t{:.3f}'.format(checkpoint[2]))\n",
    "        #print('updated final cost:\\t{:.3f}'.format(J_history[-1]))\n",
    "        #print('theta: \\n', theta)\n",
    "\n",
    "        #plt.plot(list(range(1, len(J_history) + 1)), J_history)\n",
    "        #plt.xlabel('iterations')\n",
    "        #plt.ylabel('cost')\n",
    "        \n",
    "        \n",
    "        #plt.show()\n",
    "\n",
    "        # Save trained parameters\n",
    "\n",
    "        #iterations += checkpoint[1]\n",
    "        checkpoint = np.array([theta, iterations, J_history, g], dtype=object)\n",
    "        \n",
    "        theta_list.append(theta)\n",
    "        checkpoint_list.append(checkpoint)\n",
    "\n",
    "        if (year == 2014) & (period == 'may-jun'):\n",
    "            \n",
    "            np.save('../reports/parameters/checkpoints/mle_checkpoint_may-jun_2014_1.npy', checkpoint)\n",
    "        \n",
    "        elif (year == 2014) & (period == 'jun-jul'):\n",
    "            \n",
    "            np.save('../reports/parameters/checkpoints/mle_checkpoint_jun-jul_2014_1.npy', checkpoint)\n",
    "            \n",
    "        elif (year == 2015) & (period == 'may-jun'):\n",
    "            \n",
    "            np.save('../reports/parameters/checkpoints/mle_checkpoint_may-jun_2015_1.npy', checkpoint)\n",
    "            \n",
    "        elif (year == 2015) & (period == 'jun-jul'):\n",
    "            \n",
    "            np.save('../reports/parameters/checkpoints/mle_checkpoint_jun-jul_2015_1.npy', checkpoint)\n",
    "            \n",
    "        elif (year == 2016) & (period == 'may-jun'):\n",
    "            \n",
    "            np.save('../reports/parameters/checkpoints/mle_checkpoint_may-jun_2016_1.npy', checkpoint)\n",
    "            \n",
    "        elif (year == 2016) & (period == 'jun-jul'):\n",
    "            \n",
    "            np.save('../reports/parameters/checkpoints/mle_checkpoint_jun-jul_2016_1.npy', checkpoint)\n",
    "            \n",
    "        elif (year == 2017) & (period == 'may-jun'):\n",
    "            \n",
    "            np.save('../reports/parameters/checkpoints/mle_checkpoint_may-jun_2017_1.npy', checkpoint)\n",
    "            \n",
    "        elif (year == 2017) & (period == 'jun-jul'):\n",
    "            \n",
    "            np.save('../reports/parameters/checkpoints/mle_checkpoint_jun-jul_2017_1.npy', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -1.90199427],\n",
       "       [  -5.89298669],\n",
       "       [1084.87231619],\n",
       "       [  98.22151184],\n",
       "       [ 826.58571766],\n",
       "       [4190.68361118],\n",
       "       [  12.36124891],\n",
       "       [  19.77812373],\n",
       "       [   3.90317286],\n",
       "       [   1.55499324],\n",
       "       [  -0.226496  ],\n",
       "       [   0.7442895 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.02248995, 0.00662248, 0.0326708 , 0.0218138 ,\n",
       "       0.00938308, 0.03003079, 0.01667218, 0.01701338, 0.01689585,\n",
       "       0.01862485, 0.01852546, 0.01873603, 0.01681696, 0.00678086,\n",
       "       0.02194052, 0.02566796, 0.0224682 , 0.02164904, 0.01959695,\n",
       "       0.02119097, 0.03095749, 0.02063743, 0.01938343, 0.01891767,\n",
       "       0.0191342 , 0.02087727, 0.01663278, 0.01026286, 0.01645465,\n",
       "       0.01015198, 0.01649842, 0.00765829, 0.00690899, 0.00762512,\n",
       "       0.00889952, 0.00725284, 0.00536449, 0.00467987, 0.00748682,\n",
       "       0.00642059, 0.00658914, 0.01009279, 0.0114314 , 0.01122291,\n",
       "       0.01033175, 0.01098708, 0.00901219, 0.00894552, 0.01099142,\n",
       "       0.00901247, 0.00498172, 0.00494121, 0.00945305, 0.00926055,\n",
       "       0.01047521, 0.01091334, 0.01056212, 0.00587598, 0.00558788,\n",
       "       0.00856882, 0.00437667, 0.01978611, 0.01980495, 0.01931544,\n",
       "       0.02248399, 0.0329257 , 0.02273099, 0.02492186, 0.02560626,\n",
       "       0.01736334, 0.0203856 , 0.02167503, 0.02301612, 0.03359969,\n",
       "       0.03195266, 0.02629888, 0.02948285, 0.03056413, 0.03893951,\n",
       "       0.03204557, 0.03122459, 0.0335588 , 0.01578322, 0.00900166,\n",
       "       0.01787553, 0.01755559, 0.01632447, 0.01633702, 0.01628522,\n",
       "       0.01603693, 0.01595569, 0.01802542, 0.01557779, 0.01754003,\n",
       "       0.01590125, 0.01598607, 0.015991  , 0.01672664])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_lag[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize the cost function and get the optimized initial parameter values\n",
    "            #res = minimize(costFunction, theta, method='BFGS', jac=gradient1, hess=hessian, options={'gtol': 1e-8, 'maxiter': 100000, 'disp': True})\n",
    "            #theta = res.x\n",
    "            #theta = theta.reshape(12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinEig:  -0.048339770792663e-1370\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 2\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m, \u001b[39m10000\u001b[39m, size\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Compute Hessian\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     hess \u001b[39m=\u001b[39m hessian(theta)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Compute minimum eigenvalue of the Hessian\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     min_eig_hess \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmin(np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigvalsh(hess))\n",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 2\u001b[0m in \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=428'>429</a>\u001b[0m mu \u001b[39m=\u001b[39m y \u001b[39m-\u001b[39m (n \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta(theta))))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=430'>431</a>\u001b[0m \u001b[39m# Hessian entries\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=431'>432</a>\u001b[0m hessian \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m N) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum((partial_sq(theta) \u001b[39m*\u001b[39m mu \u001b[39m-\u001b[39m n \u001b[39m*\u001b[39m (partial_by_partial(theta)) \u001b[39m*\u001b[39m (np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta(theta)) \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta(theta)))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)), axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=432'>433</a>\u001b[0m hessian \u001b[39m=\u001b[39m hessian\u001b[39m.\u001b[39mreshape((\u001b[39m12\u001b[39m, \u001b[39m12\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=434'>435</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hessian\n",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 2\u001b[0m in \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=331'>332</a>\u001b[0m d_gamma1_d_eta21_i \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtI1[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum((a_lag[mask] \u001b[39m*\u001b[39m (y_lag[mask] \u001b[39m/\u001b[39m n_lag[mask])) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta21 \u001b[39m*\u001b[39m s_lag[mask]) \u001b[39m*\u001b[39m (w_lag[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha1 \u001b[39m*\u001b[39m dist[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))) \u001b[39m*\u001b[39m sI1_lag[mask] \u001b[39m*\u001b[39m s_lag[mask])\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=332'>333</a>\u001b[0m d_gamma1_d_alpha1_i \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtI1[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum((a_lag[mask] \u001b[39m*\u001b[39m (y_lag[mask] \u001b[39m/\u001b[39m n_lag[mask])) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta21 \u001b[39m*\u001b[39m s_lag[mask]) \u001b[39m*\u001b[39m (w_lag[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha1 \u001b[39m*\u001b[39m dist[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))) \u001b[39m*\u001b[39m sI1_lag[mask] \u001b[39m*\u001b[39m dist[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=333'>334</a>\u001b[0m d_gamma2_d_eta22_i \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtI2[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum((a_lag[mask] \u001b[39m*\u001b[39m (y_lag[mask] \u001b[39m/\u001b[39m n_lag[mask])) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mexp(\u001b[39m-\u001b[39;49meta22 \u001b[39m*\u001b[39;49m s_lag[mask]) \u001b[39m*\u001b[39m (w_lag[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha2 \u001b[39m*\u001b[39m dist[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))) \u001b[39m*\u001b[39m sI2[mask] \u001b[39m*\u001b[39m s_lag[mask])\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=334'>335</a>\u001b[0m d_gamma2_d_alpha2_i \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtI2[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum((a_lag[mask] \u001b[39m*\u001b[39m (y_lag[mask] \u001b[39m/\u001b[39m n_lag[mask])) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta22 \u001b[39m*\u001b[39m s_lag[mask]) \u001b[39m*\u001b[39m (w_lag[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha2 \u001b[39m*\u001b[39m dist[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))) \u001b[39m*\u001b[39m sI2[mask] \u001b[39m*\u001b[39m dist[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#X26sZmlsZQ%3D%3D?line=335'>336</a>\u001b[0m d_alpha1_d_gamma1_i \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtI1[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum((a_lag[mask] \u001b[39m*\u001b[39m (y_lag[mask] \u001b[39m/\u001b[39m n_lag[mask])) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta21 \u001b[39m*\u001b[39m s_lag[mask]) \u001b[39m*\u001b[39m (w_lag[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha1 \u001b[39m*\u001b[39m dist[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))) \u001b[39m*\u001b[39m sI1_lag[mask] \u001b[39m*\u001b[39m dist[:, i][mask]\u001b[39m.\u001b[39mreshape(N\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_eig_hess = -1\n",
    "while min_eig_hess <= 0:\n",
    "    # Initialize fitting parameters\n",
    "     \n",
    "    theta = np.random.uniform(-10, 10000, size=(12, 1))\n",
    "    \n",
    "    try:\n",
    "        # Compute Hessian\n",
    "        hess = hessian(theta)\n",
    "\n",
    "        # Compute minimum eigenvalue of the Hessian\n",
    "        min_eig_hess = np.min(np.linalg.eigvalsh(hess))\n",
    "        print('MinEig: ', min_eig_hess, end='\\r')\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Failed to compute eigenvalues, retrying...\", end='\\r')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 25\n",
      "         Function evaluations: 1925\n",
      "         Gradient evaluations: 148\n",
      "LinAlgError: eigenvalues did not converge. Retrying...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 30.875477\n",
      "         Iterations: 25\n",
      "         Function evaluations: 599\n",
      "         Gradient evaluations: 45\n",
      "[ -0.78334465  -2.62405152  10.78212196  51.07813128  14.90650768 -0.00100571509771798\n",
      " -11.83671087   7.21130785  10.01841518   7.58676192   0.71983888\n",
      "  10.2293322   10.66165511]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 5\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(N), size\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Minimize the cost function and get the optimized parameter values\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m#res = minimize(costFunction, theta, method='BFGS', jac=gradient1, hess=hessian, options={'gtol': 1e-5, 'maxiter': 100000, 'disp': True})\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     res \u001b[39m=\u001b[39m minimize(costFunction, theta, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBFGS\u001b[39m\u001b[39m'\u001b[39m, options\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mgtol\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100000\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     theta \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mx\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     max_abs_grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(np\u001b[39m.\u001b[39mabs(gradient(theta)))\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:691\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    689\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbfgs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 691\u001b[0m     res \u001b[39m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    692\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1388\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[0;32m   1385\u001b[0m pk \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mdot(Hk, gfk)\n\u001b[0;32m   1386\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1387\u001b[0m     alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 \u001b[39m=\u001b[39m \\\n\u001b[1;32m-> 1388\u001b[0m              _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[0;32m   1389\u001b[0m                                   old_fval, old_old_fval, amin\u001b[39m=\u001b[39;49m\u001b[39m1e-100\u001b[39;49m, amax\u001b[39m=\u001b[39;49m\u001b[39m1e100\u001b[39;49m)\n\u001b[0;32m   1390\u001b[0m \u001b[39mexcept\u001b[39;00m _LineSearchError:\n\u001b[0;32m   1391\u001b[0m     \u001b[39m# Line search failed to find a better solution.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m     warnflag \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1160\u001b[0m, in \u001b[0;36m_line_search_wolfe12\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[39mSame as line_search_wolfe1, but fall back to line_search_wolfe2 if\u001b[39;00m\n\u001b[0;32m   1148\u001b[0m \u001b[39msuitable step length is not found, and raise an exception if a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \n\u001b[0;32m   1156\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m extra_condition \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mextra_condition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1160\u001b[0m ret \u001b[39m=\u001b[39m line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[0;32m   1161\u001b[0m                          old_fval, old_old_fval,\n\u001b[0;32m   1162\u001b[0m                          \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1164\u001b[0m \u001b[39mif\u001b[39;00m ret[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m extra_condition \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     xp1 \u001b[39m=\u001b[39m xk \u001b[39m+\u001b[39m ret[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m pk\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:84\u001b[0m, in \u001b[0;36mline_search_wolfe1\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(gval[\u001b[39m0\u001b[39m], pk)\n\u001b[0;32m     82\u001b[0m derphi0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(gfk, pk)\n\u001b[1;32m---> 84\u001b[0m stp, fval, old_fval \u001b[39m=\u001b[39m scalar_search_wolfe1(\n\u001b[0;32m     85\u001b[0m         phi, derphi, old_fval, old_old_fval, derphi0,\n\u001b[0;32m     86\u001b[0m         c1\u001b[39m=\u001b[39;49mc1, c2\u001b[39m=\u001b[39;49mc2, amax\u001b[39m=\u001b[39;49mamax, amin\u001b[39m=\u001b[39;49mamin, xtol\u001b[39m=\u001b[39;49mxtol)\n\u001b[0;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m stp, fc[\u001b[39m0\u001b[39m], gc[\u001b[39m0\u001b[39m], fval, old_fval, gval[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:161\u001b[0m, in \u001b[0;36mscalar_search_wolfe1\u001b[1;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m    159\u001b[0m     alpha1 \u001b[39m=\u001b[39m stp\n\u001b[0;32m    160\u001b[0m     phi1 \u001b[39m=\u001b[39m phi(stp)\n\u001b[1;32m--> 161\u001b[0m     derphi1 \u001b[39m=\u001b[39m derphi(stp)\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:78\u001b[0m, in \u001b[0;36mline_search_wolfe1.<locals>.derphi\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mderphi\u001b[39m(s):\n\u001b[1;32m---> 78\u001b[0m     gval[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m fprime(xk \u001b[39m+\u001b[39;49m s\u001b[39m*\u001b[39;49mpk, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     79\u001b[0m     gc[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(gval[\u001b[39m0\u001b[39m], pk)\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:273\u001b[0m, in \u001b[0;36mScalarFunction.grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 273\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad()\n\u001b[0;32m    274\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_grad\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated:\n\u001b[1;32m--> 256\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad_impl()\n\u001b[0;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[0;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m approx_derivative(fun_wrapped, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, f0\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf,\n\u001b[0;32m    174\u001b[0m                            \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfinite_diff_options)\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     use_one_sided \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m sparsity \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0;32m    506\u001b[0m                              use_one_sided, method)\n\u001b[0;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(sparsity) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(sparsity) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    574\u001b[0m     x \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n\u001b[0;32m    575\u001b[0m     dx \u001b[39m=\u001b[39m x[i] \u001b[39m-\u001b[39m x0[i]  \u001b[39m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m     df \u001b[39m=\u001b[39m fun(x) \u001b[39m-\u001b[39m f0\n\u001b[0;32m    577\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m3-point\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    578\u001b[0m     x1 \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun_wrapped\u001b[39m(x):\n\u001b[1;32m--> 456\u001b[0m     f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_1d(fun(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    458\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`fun` return value has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mmore than 1 dimension.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcostFunction\u001b[39m(theta): \n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     neg_log_likelihood \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mN) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(y \u001b[39m*\u001b[39m eta(theta) \u001b[39m-\u001b[39m n \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mexp(eta(theta))))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m neg_log_likelihood\n",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m dispersal_array1 \u001b[39m=\u001b[39m dispersal_array \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta21 \u001b[39m*\u001b[39m s_lag) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha1 \u001b[39m*\u001b[39m dist[:, i]\u001b[39m.\u001b[39mreshape(N,\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m sI1_lag\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m dispersal_array2 \u001b[39m=\u001b[39m dispersal_array \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta22 \u001b[39m*\u001b[39m s_lag) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha2 \u001b[39m*\u001b[39m dist[:, i]\u001b[39m.\u001b[39mreshape(N,\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m sI2\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m dispersal_component1_i \u001b[39m=\u001b[39m gamma1 \u001b[39m*\u001b[39m (np\u001b[39m.\u001b[39;49msum(dispersal_array1) \u001b[39m-\u001b[39m dispersal_array1[i][\u001b[39m0\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m dispersal_component2_i \u001b[39m=\u001b[39m gamma2 \u001b[39m*\u001b[39m (np\u001b[39m.\u001b[39msum(dispersal_array2) \u001b[39m-\u001b[39m dispersal_array2[i][\u001b[39m0\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W4sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m dispersal1\u001b[39m.\u001b[39mappend(dispersal_component1_i)\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Initialize fitting parameters\n",
    "    theta = np.random.normal(10, 10 / np.sqrt(N), size=(12, 1))\n",
    "    \n",
    "    try:\n",
    "        # Minimize the cost function and get the optimized parameter values\n",
    "        #res = minimize(costFunction, theta, method='BFGS', jac=gradient1, hess=hessian, options={'gtol': 1e-5, 'maxiter': 100000, 'disp': True})\n",
    "        res = minimize(costFunction, theta, method='BFGS', options={'gtol': 1e-5, 'maxiter': 100000, 'disp': True})\n",
    "        theta = res.x\n",
    "        \n",
    "        max_abs_grad = np.max(np.abs(gradient(theta)))\n",
    "        min_eig_hess = np.min(np.linalg.eigvalsh(hessian(theta)))\n",
    "        \n",
    "        print('Year: ', year, 'Period: ', period, 'MaxGrad: ', max_abs_grad, 'MinEig: ', min_eig_hess, end='\\r')\n",
    "        print(theta.ravel())\n",
    "        # Check custom convergence criteria\n",
    "        if (max_abs_grad < 1e-5) and (min_eig_hess > 1e-5):\n",
    "        #if (max_abs_grad < 1e-5) and np.max(np.abs(theta)) < 500000: \n",
    "            break\n",
    "    except np.linalg.LinAlgError:\n",
    "        print('LinAlgError: eigenvalues did not converge. Retrying...')\n",
    " \n",
    "theta = theta.reshape(12, 1)        \n",
    "np.save('../reports/parameters/theta_may-jun_2014_BFGS.npy', theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.02944166],\n",
       "       [68.53351592],\n",
       "       [ 0.00000282],\n",
       "       [ 0.04434991],\n",
       "       [ 0.00039621],\n",
       "       [ 0.01117746],\n",
       "       [-0.00022887],\n",
       "       [-0.0070653 ],\n",
       "       [-0.0000257 ],\n",
       "       [-0.00006961],\n",
       "       [-0.00000374],\n",
       "       [-0.00001694]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 10.520207070601893\n",
      "            Iterations: 41\n",
      "            Function evaluations: 541\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully    (Exit mode 0)607e-05 MinEig:  -3.1953376888106294e-09\n",
      "            Current function value: 10.520197917038132\n",
      "            Iterations: 49\n",
      "            Function evaluations: 651\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully    (Exit mode 0)207e-05 MinEig:  -4.018649863049439e-07\n",
      "            Current function value: 10.520208031471855\n",
      "            Iterations: 39\n",
      "            Function evaluations: 513\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully    (Exit mode 0)559e-05 MinEig:  -8.678448132843538e-10\n",
      "            Current function value: 10.520235823647093\n",
      "            Iterations: 43\n",
      "            Function evaluations: 575\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully    (Exit mode 0)917e-05 MinEig:  -9.136639652883964e-08\n",
      "            Current function value: 10.52020739021353\n",
      "            Iterations: 40\n",
      "            Function evaluations: 530\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully    (Exit mode 0)715e-05 MinEig:  -1.55177133137369e-09\n",
      "            Current function value: 10.520206883988052\n",
      "            Iterations: 36\n",
      "            Function evaluations: 475\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully    (Exit mode 0)266e-05 MinEig:  -5.855759804301595e-10\n",
      "            Current function value: 10.520205781419516\n",
      "            Iterations: 37\n",
      "            Function evaluations: 491\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully    (Exit mode 0)095e-05 MinEig:  -1.4456047251925438e-09\n",
      "            Current function value: 10.520197659864213\n",
      "            Iterations: 32\n",
      "            Function evaluations: 421\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully    (Exit mode 0)519e-05 MinEig:  -5.698301358931136e-06\n",
      "            Current function value: 10.520206885816688\n",
      "            Iterations: 34\n",
      "            Function evaluations: 445\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully    (Exit mode 0)942e-05 MinEig:  -6.850556408068066e-10\n",
      "            Current function value: 10.520202605517703\n",
      "            Iterations: 30\n",
      "            Function evaluations: 399\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully    (Exit mode 0)052e-05 MinEig:  -1.0856670671098238e-08\n",
      "            Current function value: 10.519698713815437\n",
      "            Iterations: 73\n",
      "            Function evaluations: 959\n",
      "            Gradient evaluations: 73\n",
      "Optimization terminated successfully    (Exit mode 0)6986947 MinEig:  -0.0021489188520462665\n",
      "            Current function value: 10.520205402393943\n",
      "            Iterations: 36\n",
      "            Function evaluations: 473\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully    (Exit mode 0)06e-05 MinEig:  -3.716938554533133e-09\n",
      "            Current function value: 10.52020691367187\n",
      "            Iterations: 38\n",
      "            Function evaluations: 501\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)386e-05 MinEig:  -2.1795720162224536e-09\n",
      "            Current function value: 10.520205454464314\n",
      "            Iterations: 36\n",
      "            Function evaluations: 475\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully    (Exit mode 0)771e-05 MinEig:  -1.6766764717563398e-09\n",
      "            Current function value: 10.520204377977755\n",
      "            Iterations: 32\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully    (Exit mode 0)817e-05 MinEig:  -1.6118059726223142e-09\n",
      "            Current function value: 10.520205318244289\n",
      "            Iterations: 38\n",
      "            Function evaluations: 506\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)436e-05 MinEig:  -3.8197191945990204e-09\n",
      "            Current function value: 10.520201254077413\n",
      "            Iterations: 33\n",
      "            Function evaluations: 441\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully    (Exit mode 0)356e-05 MinEig:  -1.2225842413061777e-09\n",
      "            Current function value: 10.520210128635066\n",
      "            Iterations: 38\n",
      "            Function evaluations: 502\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)747e-05 MinEig:  -1.3809892270699214e-09\n",
      "            Current function value: 10.520205181660556\n",
      "            Iterations: 34\n",
      "            Function evaluations: 447\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully    (Exit mode 0)628e-05 MinEig:  -1.1244074716331384e-09\n",
      "            Current function value: 10.520205480127823\n",
      "            Iterations: 40\n",
      "            Function evaluations: 534\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully    (Exit mode 0)506e-05 MinEig:  -3.598087295100201e-09\n",
      "            Current function value: 10.520204511423307\n",
      "            Iterations: 41\n",
      "            Function evaluations: 546\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully    (Exit mode 0)047e-05 MinEig:  -3.157145956689008e-09\n",
      "            Current function value: 10.520199891828304\n",
      "            Iterations: 38\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)96e-05 MinEig:  -2.4215337655802036e-09\n",
      "            Current function value: 10.520206946477515\n",
      "            Iterations: 30\n",
      "            Function evaluations: 402\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully    (Exit mode 0)508e-05 MinEig:  -7.44768178214253e-10\n",
      "            Current function value: 10.520206368214465\n",
      "            Iterations: 41\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully    (Exit mode 0)643e-05 MinEig:  -2.9443077271613967e-09\n",
      "            Current function value: 8.433346102174015\n",
      "            Iterations: 46\n",
      "            Function evaluations: 623\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)74e-05 MinEig:  -8.485488306952723e-06\n",
      "            Current function value: 10.520207551994657\n",
      "            Iterations: 35\n",
      "            Function evaluations: 463\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully    (Exit mode 0)688e-05 MinEig:  -7.353403354098535e-10\n",
      "            Current function value: 10.520203653217756\n",
      "            Iterations: 34\n",
      "            Function evaluations: 447\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully    (Exit mode 0)937e-05 MinEig:  -1.1892337492679764e-09\n",
      "            Current function value: 10.520202726457669\n",
      "            Iterations: 37\n",
      "            Function evaluations: 493\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully    (Exit mode 0)386e-05 MinEig:  -8.669486204393711e-10\n",
      "            Current function value: 10.520205999588258\n",
      "            Iterations: 30\n",
      "            Function evaluations: 398\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully    (Exit mode 0)206e-05 MinEig:  -2.1701779871672786e-09\n",
      "            Current function value: 10.520208980071232\n",
      "            Iterations: 35\n",
      "            Function evaluations: 463\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully    (Exit mode 0)635e-05 MinEig:  -6.873266803310635e-10\n",
      "            Current function value: 8.433341401450164\n",
      "            Iterations: 78\n",
      "            Function evaluations: 1041\n",
      "            Gradient evaluations: 78\n",
      "Optimization terminated successfully    (Exit mode 0)7927e-05 MinEig:  -1.2552320779766314e-06\n",
      "            Current function value: 10.520209351545635\n",
      "            Iterations: 39\n",
      "            Function evaluations: 516\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully    (Exit mode 0)814e-05 MinEig:  -1.149034188055828e-09\n",
      "            Current function value: 10.520209772219683\n",
      "            Iterations: 40\n",
      "            Function evaluations: 527\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully    (Exit mode 0)4e-05 MinEig:  -1.5890062273250257e-09\n",
      "            Current function value: 8.433344574649817\n",
      "            Iterations: 49\n",
      "            Function evaluations: 654\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully    (Exit mode 0)8788e-05 MinEig:  -1.6291313872875474e-06\n",
      "            Current function value: 10.520201135721544\n",
      "            Iterations: 32\n",
      "            Function evaluations: 426\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully    (Exit mode 0)155e-05 MinEig:  -2.9091843639062513e-09\n",
      "            Current function value: 10.520210002515126\n",
      "            Iterations: 37\n",
      "            Function evaluations: 488\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully    (Exit mode 0)385e-05 MinEig:  -1.3337664113228897e-09\n",
      "            Current function value: 10.520207299595796\n",
      "            Iterations: 36\n",
      "            Function evaluations: 474\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully    (Exit mode 0)003e-05 MinEig:  -8.970267400977324e-10\n",
      "            Current function value: 10.520202067025194\n",
      "            Iterations: 29\n",
      "            Function evaluations: 382\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully    (Exit mode 0)396e-05 MinEig:  -2.163356329419505e-10\n",
      "            Current function value: 10.520210062536329\n",
      "            Iterations: 37\n",
      "            Function evaluations: 487\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully    (Exit mode 0)012e-05 MinEig:  -7.90345173262965e-10\n",
      "            Current function value: 8.433350641093975\n",
      "            Iterations: 55\n",
      "            Function evaluations: 743\n",
      "            Gradient evaluations: 55\n",
      "Optimization terminated successfully    (Exit mode 0)9683e-05 MinEig:  -2.459356769755145e-05\n",
      "            Current function value: 10.520215678735354\n",
      "            Iterations: 38\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)431e-05 MinEig:  -9.224592828688095e-09\n",
      "            Current function value: 10.52020540590036\n",
      "            Iterations: 36\n",
      "            Function evaluations: 473\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully    (Exit mode 0)093e-05 MinEig:  -1.3098957027822485e-09\n",
      "            Current function value: 10.520207761027255\n",
      "            Iterations: 38\n",
      "            Function evaluations: 502\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)16e-05 MinEig:  -1.755612966198496e-09\n",
      "            Current function value: 10.520207215670547\n",
      "            Iterations: 35\n",
      "            Function evaluations: 465\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully    (Exit mode 0)999e-05 MinEig:  -1.1909253055857115e-09\n",
      "            Current function value: 10.520202295943514\n",
      "            Iterations: 35\n",
      "            Function evaluations: 465\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully    (Exit mode 0)755e-05 MinEig:  -1.569622029857848e-08\n",
      "            Current function value: 10.520205223142366\n",
      "            Iterations: 36\n",
      "            Function evaluations: 478\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully    (Exit mode 0)592e-05 MinEig:  -4.296300748595871e-09\n",
      "            Current function value: 10.520200293065269\n",
      "            Iterations: 32\n",
      "            Function evaluations: 423\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully    (Exit mode 0)134e-05 MinEig:  -7.531405913170111e-10\n",
      "            Current function value: 10.520210738193057\n",
      "            Iterations: 36\n",
      "            Function evaluations: 474\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully    (Exit mode 0)037e-05 MinEig:  -3.666093518699979e-09\n",
      "            Current function value: 10.520205164374573\n",
      "            Iterations: 35\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully    (Exit mode 0)202e-05 MinEig:  -3.623880081869564e-09\n",
      "            Current function value: 10.520118191539334\n",
      "            Iterations: 46\n",
      "            Function evaluations: 612\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)08548443 MinEig:  -0.00031959205360659984\n",
      "            Current function value: 10.52020513225132\n",
      "            Iterations: 37\n",
      "            Function evaluations: 487\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully    (Exit mode 0)313e-05 MinEig:  -3.733073662870249e-09\n",
      "            Current function value: 10.520206107551411\n",
      "            Iterations: 38\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)608e-05 MinEig:  -1.3082091197500516e-09\n",
      "            Current function value: 10.52020591754805\n",
      "            Iterations: 38\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)276e-05 MinEig:  -7.982039388507625e-10\n",
      "            Current function value: 8.43326994236728\n",
      "            Iterations: 47\n",
      "            Function evaluations: 629\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully    (Exit mode 0)677112 MinEig:  -0.0002927460911310318\n",
      "            Current function value: 10.520205562052855\n",
      "            Iterations: 34\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully    (Exit mode 0)632e-05 MinEig:  -8.429410353504993e-10\n",
      "            Current function value: 10.520201974339848\n",
      "            Iterations: 38\n",
      "            Function evaluations: 502\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)564e-05 MinEig:  -7.781877773045452e-10\n",
      "            Current function value: 10.519947462629098\n",
      "            Iterations: 62\n",
      "            Function evaluations: 831\n",
      "            Gradient evaluations: 62\n",
      "Optimization terminated successfully    (Exit mode 0)6869092 MinEig:  -0.0011188100395026419\n",
      "            Current function value: 8.43334292161315\n",
      "            Iterations: 62\n",
      "            Function evaluations: 837\n",
      "            Gradient evaluations: 62\n",
      "Optimization terminated successfully    (Exit mode 0)462e-05 MinEig:  -1.917241983061584e-09\n",
      "            Current function value: 10.520211512831324\n",
      "            Iterations: 31\n",
      "            Function evaluations: 408\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully    (Exit mode 0)808e-05 MinEig:  -2.541508459639396e-06\n",
      "            Current function value: 10.520204391966427\n",
      "            Iterations: 41\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully    (Exit mode 0)502e-05 MinEig:  -2.495566501558417e-09\n",
      "            Current function value: 8.433360346653759\n",
      "            Iterations: 44\n",
      "            Function evaluations: 580\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully    (Exit mode 0)23e-06 MinEig:  -4.808219950558582e-07\n",
      "            Current function value: 10.520207319136663\n",
      "            Iterations: 30\n",
      "            Function evaluations: 400\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully    (Exit mode 0)278e-05 MinEig:  -8.286921579547255e-10\n",
      "            Current function value: 10.520205295398334\n",
      "            Iterations: 39\n",
      "            Function evaluations: 519\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully    (Exit mode 0)333e-05 MinEig:  -1.8233149440323004e-09\n",
      "            Current function value: 10.520205413405142\n",
      "            Iterations: 40\n",
      "            Function evaluations: 532\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully    (Exit mode 0)817e-05 MinEig:  -1.7452719238633489e-09\n",
      "            Current function value: 10.52020883271408\n",
      "            Iterations: 39\n",
      "            Function evaluations: 516\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully    (Exit mode 0)672e-05 MinEig:  -1.3051181648612402e-09\n",
      "            Current function value: 8.433343324848677\n",
      "            Iterations: 35\n",
      "            Function evaluations: 457\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully    (Exit mode 0)1243e-06 MinEig:  -3.664918678820261e-08\n",
      "            Current function value: 10.520209427799324\n",
      "            Iterations: 42\n",
      "            Function evaluations: 555\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully    (Exit mode 0)524e-05 MinEig:  -1.104756404270397e-09\n",
      "            Current function value: 10.52008503191886\n",
      "            Iterations: 37\n",
      "            Function evaluations: 483\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully    (Exit mode 0)4910144 MinEig:  -0.0004989251963263802\n",
      "            Current function value: 10.520203082149166\n",
      "            Iterations: 35\n",
      "            Function evaluations: 463\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully    (Exit mode 0)866e-05 MinEig:  -9.58982442985717e-10\n",
      "            Current function value: 10.520207988922902\n",
      "            Iterations: 37\n",
      "            Function evaluations: 493\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully    (Exit mode 0)44e-05 MinEig:  -2.3655172664768045e-09\n",
      "            Current function value: 10.520202849393135\n",
      "            Iterations: 34\n",
      "            Function evaluations: 458\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully    (Exit mode 0)304e-05 MinEig:  -2.2777827589082778e-09\n",
      "            Current function value: 10.520207726629494\n",
      "            Iterations: 38\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)644e-05 MinEig:  -8.770875895374629e-10\n",
      "            Current function value: 10.52020744591581\n",
      "            Iterations: 38\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)526e-05 MinEig:  -8.988582549244701e-10\n",
      "            Current function value: 8.43334338665347\n",
      "            Iterations: 47\n",
      "            Function evaluations: 631\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully    (Exit mode 0)601e-05 MinEig:  -4.838236115737323e-06\n",
      "            Current function value: 10.520215551178953\n",
      "            Iterations: 38\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)221e-05 MinEig:  -1.3622808444671248e-08\n",
      "            Current function value: 10.520209465902385\n",
      "            Iterations: 39\n",
      "            Function evaluations: 516\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully    (Exit mode 0)485e-05 MinEig:  -2.2505575497492065e-09\n",
      "            Current function value: 10.520203934717966\n",
      "            Iterations: 34\n",
      "            Function evaluations: 449\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully    (Exit mode 0)269e-05 MinEig:  -6.3574595653645436e-09\n",
      "            Current function value: 10.520206426864563\n",
      "            Iterations: 38\n",
      "            Function evaluations: 505\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)79e-05 MinEig:  -5.260147644130358e-10\n",
      "            Current function value: 10.52020950285141\n",
      "            Iterations: 35\n",
      "            Function evaluations: 463\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully    (Exit mode 0)034e-05 MinEig:  -1.4671507125892863e-08\n",
      "            Current function value: 10.520209448795864\n",
      "            Iterations: 38\n",
      "            Function evaluations: 502\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully    (Exit mode 0)87e-05 MinEig:  -1.9415201434369205e-09\n",
      "            Current function value: 10.52020759812485\n",
      "            Iterations: 32\n",
      "            Function evaluations: 421\n",
      "            Gradient evaluations: 32\n",
      "Year:  2017 Period:  may-jun MaxGrad:  8.27638225600072e-05 MinEig:  -8.65892900758078e-10\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mravel(theta)  \u001b[39m# ensure theta is a 1-D array\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Minimize the cost function and get the optimized parameter values\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     res \u001b[39m=\u001b[39m minimize(costFunction, theta, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSLSQP\u001b[39m\u001b[39m'\u001b[39m, bounds\u001b[39m=\u001b[39mbounds, options\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mftol\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-10\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100000\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     theta \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mx\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     max_abs_grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(np\u001b[39m.\u001b[39mabs(gradient(theta)))\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:705\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    703\u001b[0m                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mslsqp\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 705\u001b[0m     res \u001b[39m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m    706\u001b[0m                           constraints, callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    707\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrust-constr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    708\u001b[0m     res \u001b[39m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    709\u001b[0m                                        bounds, constraints,\n\u001b[0;32m    710\u001b[0m                                        callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_slsqp_py.py:432\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    429\u001b[0m     c \u001b[39m=\u001b[39m _eval_constraint(x, cons)\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:  \u001b[39m# gradient evaluation required\u001b[39;00m\n\u001b[1;32m--> 432\u001b[0m     g \u001b[39m=\u001b[39m append(wrapped_grad(x), \u001b[39m0.0\u001b[39m)\n\u001b[0;32m    433\u001b[0m     a \u001b[39m=\u001b[39m _eval_con_normals(x, cons, la, n, m, meq, mieq)\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m majiter \u001b[39m>\u001b[39m majiter_prev:\n\u001b[0;32m    436\u001b[0m     \u001b[39m# call callback if major iteration has incremented\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:346\u001b[0m, in \u001b[0;36m_clip_x_for_func.<locals>.eval\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval\u001b[39m(x):\n\u001b[0;32m    345\u001b[0m     x \u001b[39m=\u001b[39m _check_clip_x(x, bounds)\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m func(x)\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:273\u001b[0m, in \u001b[0;36mScalarFunction.grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 273\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad()\n\u001b[0;32m    274\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_grad\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated:\n\u001b[1;32m--> 256\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad_impl()\n\u001b[0;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[0;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m approx_derivative(fun_wrapped, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, f0\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf,\n\u001b[0;32m    174\u001b[0m                            \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfinite_diff_options)\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     use_one_sided \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m sparsity \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0;32m    506\u001b[0m                              use_one_sided, method)\n\u001b[0;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(sparsity) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(sparsity) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    574\u001b[0m     x \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n\u001b[0;32m    575\u001b[0m     dx \u001b[39m=\u001b[39m x[i] \u001b[39m-\u001b[39m x0[i]  \u001b[39m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m     df \u001b[39m=\u001b[39m fun(x) \u001b[39m-\u001b[39m f0\n\u001b[0;32m    577\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m3-point\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    578\u001b[0m     x1 \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun_wrapped\u001b[39m(x):\n\u001b[1;32m--> 456\u001b[0m     f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_1d(fun(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    458\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`fun` return value has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mmore than 1 dimension.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joshf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcostFunction\u001b[39m(theta): \n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     neg_log_likelihood \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mN) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(y \u001b[39m*\u001b[39m eta(theta) \u001b[39m-\u001b[39m n \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mexp(eta(theta))))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m neg_log_likelihood\n",
      "\u001b[1;32me:\\Google Drive\\Github\\hops-policy-optimization\\notebooks\\2.2-mle.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m dispersal_array \u001b[39m=\u001b[39m ((a_lag \u001b[39m*\u001b[39m (y_lag \u001b[39m/\u001b[39m n_lag)) \u001b[39m*\u001b[39m (w_lag[:, i]\u001b[39m.\u001b[39mreshape(N,\u001b[39m1\u001b[39m)))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m dispersal_array1 \u001b[39m=\u001b[39m dispersal_array \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39meta21 \u001b[39m*\u001b[39m s_lag) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha1 \u001b[39m*\u001b[39m dist[:, i]\u001b[39m.\u001b[39mreshape(N,\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m sI1_lag\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m dispersal_array2 \u001b[39m=\u001b[39m dispersal_array \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mexp(\u001b[39m-\u001b[39;49meta22 \u001b[39m*\u001b[39;49m s_lag) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39malpha2 \u001b[39m*\u001b[39m dist[:, i]\u001b[39m.\u001b[39mreshape(N,\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m sI2\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m dispersal_component1_i \u001b[39m=\u001b[39m gamma1 \u001b[39m*\u001b[39m (np\u001b[39m.\u001b[39msum(dispersal_array1) \u001b[39m-\u001b[39m dispersal_array1[i][\u001b[39m0\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Google%20Drive/Github/hops-policy-optimization/notebooks/2.2-mle.ipynb#W5sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m dispersal_component2_i \u001b[39m=\u001b[39m gamma2 \u001b[39m*\u001b[39m (np\u001b[39m.\u001b[39msum(dispersal_array2) \u001b[39m-\u001b[39m dispersal_array2[i][\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "\n",
    "# Define the bounds for theta parameters\n",
    "# Indices for beta1 and beta2 are [0, 1], they should be between -100 and 100\n",
    "\n",
    "# Lower bounds (-inf for no bound)\n",
    "lower_bounds = np.full(theta.size, -np.inf)\n",
    "lower_bounds[2:] = 0  # All parameters from delta to eta should be non-negative\n",
    "lower_bounds[:2] = -100  # lower bounds for beta1 and beta2\n",
    "\n",
    "# Upper bounds (inf for no bound)\n",
    "upper_bounds = np.full(theta.size, np.inf)\n",
    "upper_bounds[:2] = 100  # upper bounds for beta1 and beta2\n",
    "\n",
    "bounds = Bounds(lower_bounds, upper_bounds)  # apply bounds\n",
    "\n",
    "while True:\n",
    "    # Initialize fitting parameters\n",
    "    theta = np.random.normal(np.random.uniform(0, 100), 100 / np.sqrt(N), size=(12, 1))\n",
    "    theta = np.ravel(theta)  # ensure theta is a 1-D array\n",
    "    \n",
    "    try:\n",
    "        # Minimize the cost function and get the optimized parameter values\n",
    "        res = minimize(costFunction, theta, method='SLSQP', bounds=bounds, options={'ftol': 1e-10, 'maxiter': 100000, 'disp': True})\n",
    "        theta = res.x\n",
    "        \n",
    "        max_abs_grad = np.max(np.abs(gradient(theta)))\n",
    "        min_eig_hess = np.min(np.linalg.eigvalsh(hessian(theta)))\n",
    "        \n",
    "        print('Year: ', year, 'Period: ', period, 'MaxGrad: ', max_abs_grad, 'MinEig: ', min_eig_hess, end='\\r')\n",
    "        \n",
    "        # Check custom convergence criteria\n",
    "        if (max_abs_grad < 1e-4) and (min_eig_hess > 1e-8):\n",
    "            break\n",
    "    except np.linalg.LinAlgError:\n",
    "        print('LinAlgError: eigenvalues did not converge. Retrying...')\n",
    "\n",
    "theta = theta.reshape(12, 1)        \n",
    "np.save('../reports/parameters/theta_may-jun_2014_SLSQP.npy', theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 1157, function evaluations: 15678, CG iterations: 3841, optimality: 2.08e-07, constraint violation: 0.00e+00, execution time: 1e+02 s.\n",
      "`gtol` termination condition is satisfied.75151055327026e-05 MinEig:  -2.636271578424329e-09\n",
      "Number of iterations: 502, function evaluations: 6929, CG iterations: 1776, optimality: 3.08e-09, constraint violation: 0.00e+00, execution time: 4.5e+01 s.\n",
      "Year:  2017 Period:  may-jun MaxGrad:  8.275135712407697e-05 MinEig:  -3.601133273352982e-09\r"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "\n",
    "# Define the bounds for theta parameters\n",
    "# Indices for beta1 and beta2 are [0, 1], they should be between -100 and 100\n",
    "\n",
    "# Lower bounds (-inf for no bound)\n",
    "lower_bounds = np.full(theta.size, -np.inf)\n",
    "lower_bounds[2:] = 0  # All parameters from delta to eta should be non-negative\n",
    "lower_bounds[:2] = -100  # lower bounds for beta1 and beta2\n",
    "\n",
    "# Upper bounds (inf for no bound)\n",
    "upper_bounds = np.full(theta.size, np.inf)\n",
    "upper_bounds[:2] = 100  # upper bounds for beta1 and beta2\n",
    "\n",
    "bounds = Bounds(lower_bounds, upper_bounds)  # apply bounds\n",
    "\n",
    "while True:\n",
    "    # Initialize fitting parameters\n",
    "    theta = np.random.normal(10, 100 / np.sqrt(N), size=(12, 1))\n",
    "    theta = np.ravel(theta)  # ensure theta is a 1-D array\n",
    "    \n",
    "    try:\n",
    "        # Minimize the cost function and get the optimized parameter values\n",
    "        res = minimize(costFunction, theta, method='trust-constr', bounds=bounds, options={'xtol': 1e-6, 'maxiter': 100000, 'disp': True})\n",
    "        theta = res.x\n",
    "        \n",
    "        max_abs_grad = np.max(np.abs(gradient(theta)))\n",
    "        min_eig_hess = np.min(np.linalg.eigvalsh(hessian(theta)))\n",
    "        \n",
    "        print('Year: ', year, 'Period: ', period, 'MaxGrad: ', max_abs_grad, 'MinEig: ', min_eig_hess, end='\\r')\n",
    "        \n",
    "        # Check custom convergence criteria\n",
    "        if (max_abs_grad < 1e-5) and (min_eig_hess > 1e-8):\n",
    "            break\n",
    "    except np.linalg.LinAlgError:\n",
    "        print('LinAlgError: eigenvalues did not converge. Retrying...')\n",
    "\n",
    "theta = theta.reshape(12, 1)        \n",
    "np.save('../reports/parameters/theta_may-jun_2014_trust.npy', theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.59256371, 51.72961433, 10.34049695, 44.71963792, 40.93659022,\n",
       "       29.47460257, 38.31206175, 28.92169189, 37.22852699, 30.93316521,\n",
       "       48.38964612, 35.82492162])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43.94539231],\n",
       "       [151.80790582],\n",
       "       [  0.00067848],\n",
       "       [  0.00815129],\n",
       "       [  0.04181238],\n",
       "       [  0.00010497],\n",
       "       [ -0.37976637],\n",
       "       [ -0.00006866],\n",
       "       [ -0.008642  ],\n",
       "       [ -0.        ],\n",
       "       [ -1.40044642],\n",
       "       [ -0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(theta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "$$\n",
    "\\eta_{i}=\\sum_{k=1}^{K} I_{k}^{(t)}(i)\\left[\\beta_{k}+\\delta_{k}\\left(\\frac{\\tilde{y}_{i}}{n_{\\tilde{y}_{i}}} \\exp{\\left(-\\eta_{1k} s_{i}\\right)}\\right)+\\gamma_{k} \\sum_{j=1}^{M_{i}}\\left(\\frac{a_{j} z_{j}}{n_{z_{j}}} \\exp{\\left(-\\eta_{2k} s_{j}\\right)} w_{i j} \\exp{\\left(-\\alpha_{k} d_{i j}\\right)} I_{k}^{(s)}(j)\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\eta_{i}}{\\partial \\beta_{k}} &= I_{k}^{(t)}(i) \\\\\n",
    "\n",
    "\\frac{\\partial \\eta_{i}}{\\partial \\delta_{k}} &= I_{k}^{(t)}(i)\\left(\\frac{\\tilde{y}_{i}}{n_{\\tilde{y}_{i}}}\\right) \\exp \\left(-\\eta_{1 k} s_{i}\\right) \\\\\n",
    "\n",
    "\\frac{\\partial \\eta_{i}}{\\partial \\eta_{1 k}} &= -I_{k}^{(t)}(i) \\delta_{k} s_{i}\\left(\\frac{\\tilde{y}_{i}}{n_{\\tilde{y}_{i}}}\\right) \\exp \\left(-\\eta_{1 k} s_{i}\\right) \\\\\n",
    "\n",
    "\\frac{\\partial \\eta_{i}}{\\partial \\eta_{2 k}} &= -\\gamma_{k} I_{k}^{(t)}(i) \\sum_{j=1}^{M_{i}}\\left[\\left(\\frac{a_{j} z_{j}}{n_{z j}}\\right) \\exp \\left(-\\eta_{2 k} s_{j}\\right) w_{i j} \\exp \\left(-\\alpha_{k} d_{i j}\\right) I_{k}^{(s)}(j) s_{j}\\right] \\\\\n",
    "\n",
    "\\frac{\\partial \\eta_{i}}{\\partial \\gamma_{k}} &= I_{k}^{(t)}(i) \\sum_{j=1}^{M_{i}}\\left[\\left(\\frac{a_{j} z_{j}}{n_{z_{j}}}\\right) \\exp \\left(-\\eta_{2 k} s_{j}\\right) w_{i j} \\exp \\left(-\\alpha_{k} d_{i j}\\right) I_{k}^{(s)}(j)\\right] \\\\\n",
    "\n",
    "\\frac{\\partial \\eta_{i}}{\\partial \\alpha_{k}} &= -\\gamma_{k} I_{k}^{(t)}(i) \\sum_{j=1}^{M_{i}}\\left[\\left(\\frac{a_{j} z_{j}}{n_{z_{j}}}\\right) \\exp \\left(-\\eta_{2 k} s_{j}\\right) w_{i j} \\exp \\left(-\\alpha_{k} d_{i j}\\right) I_{k}^{(s)}(j) d_{i j}\\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "$$J(\\theta) = \n",
    "-\\frac{1}{N} \\sum_{i=1}^{N} y_{i} \\eta_{i}-n_{i} \\log \\left(1+e^{\\eta_{i}}\\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta} =\n",
    "-\\frac{1}{N}\\sum_{i=1}^{N} \\frac{\\partial \\eta_{i}}{\\partial \\theta}\\left(y_{i}-\\frac{n_{i}}{1+e^{-\\eta_{i}}}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### Hessian\n",
    "\n",
    "$$\\frac{\\partial^2 J}{\\partial \\theta^2} = -\\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\\left[\\frac{\\partial^{2} \\eta_{i}}{\\partial \\theta^{2}}\\left(y_{i}-\\frac{n_{i}}{1+e^{-\\eta_{i}}}\\right)- n_{i} \\left(\\frac{\\partial \\eta_{i}}{\\partial \\theta}\\right)^{2}\\frac{e^{-\\eta_{i}}}{\\left(1+e^{-\\eta_{i}}\\right)^2}\\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = np.array([theta, iterations, J_history], dtype=object)\n",
    "#np.save('../reports/checkpoint_June_July2_backup.npy', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = np.load('../reports/checkpoint_June_July2.npy', allow_pickle=True)\n",
    "theta = checkpoint[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1.5457632 ],\n",
       "       [   -0.07547458],\n",
       "       [   -4.8742034 ],\n",
       "       [   51.98424236],\n",
       "       [-3122.39869964],\n",
       "       [-4194.55086314],\n",
       "       [   -5.29015772],\n",
       "       [   -5.85815199],\n",
       "       [   -6.70538574],\n",
       "       [   -0.91393116],\n",
       "       [   -0.59329742],\n",
       "       [    0.02099419]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "$$\n",
    "\\eta_{i}=\\sum_{k=1}^{K} I_{k}^{(t)}(i)\\left[\\beta_{k}+\\delta_{k}\\left(\\frac{\\tilde{y}_{i}}{n_{\\tilde{y}_{i}}} \\exp{\\left(-\\eta_{1k} s_{i}\\right)}\\right)+\\gamma_{k} \\sum_{j=1}^{M_{i}}\\left(\\frac{a_{j} z_{j}}{n_{z_{j}}} \\exp{\\left(-\\eta_{2k} s_{j}\\right)} w_{i j} \\exp{\\left(-\\alpha_{k} d_{i j}\\right)} I_{k}^{(s)}(j)\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta_1 = -2.89902093$$\n",
    "$$\\beta_2 = -4.33376942$$\n",
    "$$\\delta_1 = 3.86406603$$\n",
    "$$\\delta_2 = 7.1830044$$\n",
    "$$\\gamma_1 = 0.06209235$$\n",
    "$$\\gamma_2 = 6.21221296$$\n",
    "$$\\alpha_1 = 0.17578305$$\n",
    "$$\\alpha_2 = 1.31264131$$\n",
    "$$\\eta_{11} = 0.13978209$$\n",
    "$$\\eta_{12} = 0.40521989$$\n",
    "$$\\eta_{21} = -0.79182359$$\n",
    "$$\\eta_{22} = 0.55742334$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues of Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,    0.00000037,    0.00000144,    0.00000554,\n",
       "          0.00212706,    0.00660576,    0.02351497,    0.40513871,\n",
       "          2.03226417,    3.69983217,  715.52475767, 2615.2486514 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigvalsh(hessian(theta_numpy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00035165],\n",
       "       [ 0.00000092],\n",
       "       [ 0.00093506],\n",
       "       [-0.00000046],\n",
       "       [ 0.00370447],\n",
       "       [-0.00027325],\n",
       "       [-0.00982039],\n",
       "       [-0.00226836],\n",
       "       [-0.00229518],\n",
       "       [ 0.00000655],\n",
       "       [-0.00047533],\n",
       "       [ 0.00037915]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGrad = nd.Gradient(costFunction)\n",
    "nHess = nd.Hessian(costFunction)\n",
    "hess_theta = nHess(theta.reshape(12,))\n",
    "grad_theta = nGrad(theta.reshape(12,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(theta):\n",
    "    \n",
    "    p = 1 / (1 + np.exp(-eta(theta)))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated probability of disease: \n",
      " [[0.02611759]\n",
      " [0.02631787]\n",
      " [0.02620498]\n",
      " [0.03368977]\n",
      " [0.02623533]\n",
      " [0.02624147]\n",
      " [0.02745905]\n",
      " [0.02710209]\n",
      " [0.04514933]\n",
      " [0.04289552]\n",
      " [0.16258389]\n",
      " [0.03171659]\n",
      " [0.15754236]\n",
      " [0.03329544]\n",
      " [0.02628519]\n",
      " [0.02748307]\n",
      " [0.02906667]\n",
      " [0.02786479]\n",
      " [0.02657169]\n",
      " [0.02892042]\n",
      " [0.02622809]\n",
      " [0.04582204]\n",
      " [0.5       ]\n",
      " [0.84480418]\n",
      " [0.24196183]\n",
      " [0.2553849 ]\n",
      " [0.33620011]\n",
      " [0.02684969]\n",
      " [0.49306239]\n",
      " [0.80226981]\n",
      " [0.53574716]\n",
      " [0.41690551]\n",
      " [0.91858263]\n",
      " [0.14165495]\n",
      " [0.        ]\n",
      " [0.02439342]\n",
      " [0.02535986]\n",
      " [0.02457473]\n",
      " [0.02567178]\n",
      " [0.17610494]\n",
      " [0.02407955]\n",
      " [0.01639678]\n",
      " [0.01495841]\n",
      " [0.02234735]\n",
      " [0.02345555]\n",
      " [0.02276539]\n",
      " [0.5       ]\n",
      " [0.02272612]\n",
      " [0.02457618]\n",
      " [0.11173597]\n",
      " [0.10794024]\n",
      " [0.22234288]\n",
      " [0.14008519]\n",
      " [0.2470191 ]\n",
      " [0.24700966]\n",
      " [0.02734554]\n",
      " [0.25025533]\n",
      " [0.02731197]\n",
      " [0.2534015 ]\n",
      " [0.02732704]\n",
      " [0.24922973]\n",
      " [0.02735182]\n",
      " [0.02728799]\n",
      " [0.02733912]\n",
      " [0.02321656]\n",
      " [0.02306974]\n",
      " [0.02242573]\n",
      " [0.5       ]\n",
      " [0.02569351]\n",
      " [0.03225039]\n",
      " [0.02612945]\n",
      " [0.02468067]\n",
      " [0.03255333]\n",
      " [0.02444586]\n",
      " [0.02351304]\n",
      " [0.0272861 ]\n",
      " [0.02401747]\n",
      " [0.02689776]\n",
      " [0.02370852]\n",
      " [0.02900357]\n",
      " [0.00353733]\n",
      " [0.5       ]\n",
      " [0.02440745]\n",
      " [0.02376449]\n",
      " [0.04679354]\n",
      " [0.02470723]\n",
      " [0.0245882 ]\n",
      " [0.02643633]\n",
      " [0.02668546]\n",
      " [0.5       ]\n",
      " [0.21687985]\n",
      " [0.28488398]\n",
      " [0.02051474]\n",
      " [0.02348036]\n",
      " [0.023665  ]\n",
      " [0.15249304]\n",
      " [0.40881938]\n",
      " [0.32363467]\n",
      " [0.02337007]\n",
      " [0.43608561]\n",
      " [0.02784324]\n",
      " [0.23166782]\n",
      " [0.03145925]\n",
      " [0.7606593 ]]\n"
     ]
    }
   ],
   "source": [
    "print('estimated probability of disease: \\n', prob(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob(theta).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
